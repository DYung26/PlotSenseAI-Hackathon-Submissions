name: Deploy GitHub Pages

on:
  push:
    branches: [ main ]
  schedule:
    # Update every hour
    - cron: '0 * * * *'
  workflow_dispatch:

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      pages: write
      id-token: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests python-dateutil

    - name: Generate submission data
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python << 'EOF'
        import json
        import os
        import glob
        import requests
        from pathlib import Path
        from datetime import datetime, timezone

        def fetch_prs_for_submissions():
            """Fetch all PRs targeting review branch and map them to submission files"""
            token = os.environ.get('GITHUB_TOKEN')
            repo = os.environ.get('GITHUB_REPOSITORY', 'PlotSenseAI/PlotSenseAI-Hackathon-Submissions')

            headers = {'Authorization': f'token {token}'} if token else {}

            pr_map = {}
            try:
                # Fetch all PRs (open and closed)
                for state in ['open', 'closed']:
                    url = f'https://api.github.com/repos/{repo}/pulls?state={state}&base=review&per_page=100'
                    response = requests.get(url, headers=headers)

                    if response.status_code == 200:
                        prs = response.json()

                        for pr in prs:
                            # Fetch PR files to find which submission it modifies
                            files_url = pr['url'] + '/files'
                            files_response = requests.get(files_url, headers=headers)

                            if files_response.status_code == 200:
                                files = files_response.json()

                                for file in files:
                                    if file['filename'].startswith('submissions/') and file['filename'].endswith('.json'):
                                        if file['filename'] not in pr_map:
                                            pr_map[file['filename']] = []

                                        pr_status = 'merged' if pr['merged_at'] else ('open' if pr['state'] == 'open' else 'closed')

                                        pr_map[file['filename']].append({
                                            'number': pr['number'],
                                            'url': pr['html_url'],
                                            'status': pr_status,
                                            'title': pr['title'],
                                            'created_at': pr['created_at'],
                                            'merged_at': pr.get('merged_at')
                                        })
            except Exception as e:
                print(f"Error fetching PRs: {e}")

            return pr_map

        def collect_submissions(pr_map):
            submissions = []
            submission_files = []
            for track_dir in ['submissions/plotsense-2025-ml', 'submissions/plotsense-2025-dev']:
                if os.path.exists(track_dir):
                    for file_path in glob.glob(f"{track_dir}/*.json"):
                        if not file_path.endswith('TEMPLATE.json'):
                            submission_files.append(file_path)

            for file_path in submission_files:
                try:
                    with open(file_path, 'r') as f:
                        submission = json.load(f)

                    file_stat = os.stat(file_path)
                    submission['submitted_date'] = datetime.fromtimestamp(
                        file_stat.st_mtime, tz=timezone.utc
                    ).isoformat()
                    submission['file_path'] = file_path

                    if 'tech_stack' not in submission:
                        if submission.get('track') == 'PlotSense ML':
                            submission['tech_stack'] = ['Python', 'PlotSenseAI', 'Pandas', 'Scikit-learn']
                        else:
                            submission['tech_stack'] = ['JavaScript', 'PlotSenseAI', 'React', 'Node.js']

                    # Get PR information for this submission
                    prs = pr_map.get(file_path, [])
                    if prs:
                        # Get the most recent PR
                        latest_pr = max(prs, key=lambda x: x['created_at'])
                        submission['status'] = latest_pr['status']
                        submission['pr_url'] = latest_pr['url']
                        submission['pr_number'] = latest_pr['number']
                    else:
                        submission['status'] = 'pending'

                    submissions.append(submission)

                except (json.JSONDecodeError, FileNotFoundError) as e:
                    print(f"Error processing {file_path}: {e}")
                    continue

            return submissions

        def generate_metrics(submissions):
            if not submissions:
                return {
                    'total_submissions': 0,
                    'total_teams': 0,
                    'tracks': {'PlotSense ML': 0, 'PlotSense Dev': 0},
                    'status_counts': {'validated': 0, 'pending': 0, 'failed': 0},
                    'tech_stack_usage': {},
                    'team_sizes': {},
                    'submission_timeline': []
                }

            metrics = {
                'total_submissions': len(submissions),
                'total_teams': len(submissions),
                'tracks': {},
                'status_counts': {'validated': 0, 'pending': 0, 'failed': 0},
                'tech_stack_usage': {},
                'team_sizes': {},
                'submission_timeline': []
            }

            for track in ['PlotSense ML', 'PlotSense Dev']:
                metrics['tracks'][track] = len([s for s in submissions if s.get('track') == track])

            for status in ['validated', 'pending', 'failed']:
                metrics['status_counts'][status] = len([s for s in submissions if s.get('status') == status])

            for submission in submissions:
                if 'tech_stack' in submission:
                    for tech in submission['tech_stack']:
                        metrics['tech_stack_usage'][tech] = metrics['tech_stack_usage'].get(tech, 0) + 1

            for submission in submissions:
                if 'team_members' in submission:
                    size = len(submission['team_members'])
                    metrics['team_sizes'][str(size)] = metrics['team_sizes'].get(str(size), 0) + 1

            timeline = {}
            for submission in submissions:
                if 'submitted_date' in submission:
                    date = submission['submitted_date'][:10]
                    timeline[date] = timeline.get(date, 0) + 1

            metrics['submission_timeline'] = [{'date': date, 'count': count} for date, count in sorted(timeline.items())]

            return metrics

        # Fetch PR data from GitHub API
        print("Fetching PR data from GitHub...")
        pr_map = fetch_prs_for_submissions()
        print(f"Found PRs for {len(pr_map)} submission files")

        # Collect September 2025 submissions
        submissions = collect_submissions(pr_map)
        metrics = generate_metrics(submissions)

        # Create directory structure for September 2025 hackathon
        os.makedirs('docs/data/september-2025', exist_ok=True)

        # Save September 2025 data
        with open('docs/data/september-2025/submissions.json', 'w') as f:
            json.dump(submissions, f, indent=2, default=str)

        with open('docs/data/september-2025/metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2, default=str)

        # Also save to legacy locations for backward compatibility
        os.makedirs('docs/data', exist_ok=True)
        with open('docs/data/submissions.json', 'w') as f:
            json.dump(submissions, f, indent=2, default=str)

        with open('docs/data/metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2, default=str)

        print(f"Processed {len(submissions)} submissions for September 2025")
        print(f"Generated metrics: {metrics['total_submissions']} total submissions")

        # Future hackathons can be added here with similar logic
        # For example:
        # future_submissions = collect_future_submissions()
        # os.makedirs('docs/data/future-hackathon', exist_ok=True)
        # with open('docs/data/future-hackathon/submissions.json', 'w') as f:
        #     json.dump(future_submissions, f, indent=2, default=str)
        EOF

    - name: Setup GitHub Pages
      uses: actions/configure-pages@v4

    - name: Upload artifacts
      uses: actions/upload-pages-artifact@v3
      with:
        path: 'docs'

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4